# Bhasha-Rupantarika
**Algorithm-Hardware Co-design approach for Multi-lingual Neural Machine Translation**

[![MIT License](https://img.shields.io/badge/License-MIT-green.svg)](https://choosealicense.com/licenses/mit/)
[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![Jupyter](https://img.shields.io/badge/Jupyter-Notebook-orange.svg)](https://jupyter.org/)
[![SystemVerilog](https://img.shields.io/badge/Hardware-SystemVerilog-red.svg)](https://en.wikipedia.org/wiki/SystemVerilog)

> *Bhasha-Rupantarika* (à¤­à¤¾à¤·à¤¾-à¤°à¥‚à¤ªà¤¾à¤¨à¥à¤¤à¤°à¤¿à¤•à¤¾) means "Language Transformer" in Sanskrit, representing our mission to bridge linguistic barriers through innovative hardware-software co-design.
<p align="center">
  <a href="https://mukullokhande99.github.io/Bhasha-Rupantarika/">
    <img src="https://img.shields.io/badge/ğŸš€%20For%20Detailed%20Description-Click%20Here-blue?style=for-the-badge&logo=github" />
  </a>
</p>

  


## ğŸš€ Overview

Bhasha-Rupantarika is a comprehensive research project that explores **algorithm-hardware co-design** methodologies for efficient multilingual neural machine translation (NMT). This project addresses the computational challenges of deploying high-quality translation models by optimizing both software algorithms and hardware architectures.

### ğŸ¯ Key Features

- ğŸ”§ **Hardware-Software Co-optimization**: Integrated design approach for NMT acceleration
- ğŸŒ **Multilingual Support**: Focused on Indian languages and cross-lingual translation
- âš¡ **Performance Optimization**: Quantization, pruning, and hardware-aware optimizations
- ğŸ“± **Edge Deployment**: Efficient inference on resource-constrained devices
- ğŸ”¬ **Research-Grade Implementation**: Comprehensive evaluation and benchmarking

  <table>
  <tr>
    <td><img src="images/img1.jpeg" width="300" alt="Image 1"></td>
    <td><img src="images/img3.jpeg" width="300" alt="Image 2"></td>
    <td><img src="images/img2.jpeg" width="300" alt="Image 3"></td>
  </tr>
</table>


## ğŸ“Š Performance Highlights

| Metric | Achievement |
|--------|-------------|
| **BLEU Score** | 30.2+ (Multilingual) |
| **Model Compression** | 8x reduction with INT4 quantization |
| **Inference Speedup** | 2.25x faster inference |
| **Memory Efficiency** | 75% reduction in GPU memory usage |
| **Supported Languages** | 200+ language pairs via NLLB-200 |

<img src="images/nllb-200-600m.jpg" width="480" >

## ğŸ› ï¸ Installation & Setup

### Prerequisites

- Python 3.8 or higher
- CUDA 11.0+ (for GPU acceleration)
- Git
- SystemVerilog simulator (optional, for hardware components)

### Quick Installation
```bash
git clone https://github.com/mukullokhande99/Bhasha-Rupantarika.git
cd Bhasha-Rupantarika
pip install -r requirements.txt
```
### For Specific Language
Every supported language by nllb-200. Language code is provided for reference.
[Language code list](nllb_languages.pdf) 

## ğŸ“Š Hardware Requirements

### Minimum Requirements
- **CPU**: Intel i5 or AMD Ryzen 5
- **RAM**: 8 GB
- **GPU**: NVIDIA L4 
- **Storage**: 10 GB free space
---

**ğŸŒŸ Star this repository if you find it helpful! ğŸŒŸ**

